{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analiseSentimentos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se6D1PuQ1qoV"
      },
      "source": [
        "#Métodos do Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcRc94aFi4lZ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "class FileToDataframe:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "  \n",
        "    # converte o arquivo para dataframe\n",
        "    def ConvertToDataframe(self, dataset):\n",
        "        try:\n",
        "            df = pd.read_csv(dataset, sep=\"\\t\", header=None, names=['text','sent'])\n",
        "            df.text = df.text.astype(str)\n",
        "            sent = [1,-1,0,\"1\",\"-1\",\"0\"]\n",
        "\n",
        "            for i in range(len(df)):\n",
        "                if df['sent'][i] in sent:\n",
        "                    df.loc[i, 'sent'] = int(df['sent'][i])\n",
        "\n",
        "            return df\n",
        "        except IOerror as exc:\n",
        "            print(\"Erro ao abrir arquivo\")\n",
        "            if exc.errno != errno.EISDIR:\n",
        "                raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9tjDdKBFDz_"
      },
      "source": [
        "class RemocaoStopwords:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    # remove as stopwords utilizando a lista do NLTK\n",
        "    def stopwordsNLTK(self, dataset):\n",
        "        import nltk\n",
        "\n",
        "        nltk.download('stopwords')\n",
        "        nltk_stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "        # remove palavras com sentido negativo\n",
        "        nltk_stopwords = [i for i in nltk_stopwords if i not in [\"não\", \"nenhum\", \"nada\", \"jamais\", \"nunca\", \"nem\"]]\n",
        "\n",
        "        for i in range(len(dataset)):\n",
        "            word = dataset['text'][i].split(' ')\n",
        "            listWords = [j for j in word if j not in nltk_stopwords]\n",
        "            dataset.loc[i, 'text'] = ' '.join(listWords)\n",
        "\n",
        "\n",
        "    # remove as stopwords utilizando a lista do Spacy\n",
        "    def stopwordsSpacy(self, dataset):\n",
        "        import spacy.cli, spacy\n",
        "\n",
        "        spacy.cli.download(\"pt_core_news_sm\")\n",
        "        sp = spacy.load(\"pt_core_news_sm\")\n",
        "        spacy_stopwords = spacy.lang.pt.stop_words.STOP_WORDS\n",
        "\n",
        "        # remove palavras com sentido negativo\n",
        "        spacy_stopwords = [i for i in spacy_stopwords if i not in [\"não\", \"nenhum\", \"nada\", \"jamais\", \"nunca\", \"nem\"]]\n",
        "\n",
        "        for i in range(len(dataset)):\n",
        "            word = dataset['text'][i].split(' ')\n",
        "            listWords = [j for j in word if j not in spacy_stopwords]\n",
        "            dataset.loc[i, 'text'] = ' '.join(listWords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFg195IrHIqd"
      },
      "source": [
        "class Stemming:\n",
        "  \n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "\n",
        "    # reduz a palavra para o radical utilizando RSLP Stemmer\n",
        "    def RSLP(self, dataset):\n",
        "        import nltk\n",
        "\n",
        "        nltk.download('rslp')\n",
        "        stemmer = nltk.stem.RSLPStemmer()\n",
        "\n",
        "        for i in range(len(dataset)):\n",
        "            word = dataset['text'][i].split(' ')\n",
        "            listWords = [stemmer.stem(j) for j in word if len(j) > 1]\n",
        "            dataset.loc[i, 'text'] = ' '.join(listWords)\n",
        "\n",
        "\n",
        "    # reduz a palavra para o radical utilizando o Snowball\n",
        "    def Snowball(self, dataset):\n",
        "        import nltk\n",
        "\n",
        "        stemmer = nltk.stem.SnowballStemmer(\"portuguese\")\n",
        "\n",
        "        for i in range(len(dataset)):\n",
        "            word = dataset['text'][i].split(' ')\n",
        "            listWords = [stemmer.stem(j) for j in word if len(j) > 1]\n",
        "            dataset.loc[i, 'text'] = ' '.join(listWords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOFUaTooEZz8"
      },
      "source": [
        "!pip install symspellpy\n",
        "\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "class CorrecaoOrtografica:\n",
        "  \n",
        "    def __init__(self):\n",
        "        self.sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "        self.sym_spell.load_dictionary(\"pt_br_50k.txt\", term_index=0, count_index=1)\n",
        "\n",
        "    # corrige a ortografia de cada palavra contida no texto\n",
        "    def corrigir(self, dataset):\n",
        "        max_edit_distance = 2\n",
        "        for i in range(len(dataset)):\n",
        "            sugestoes = self.sym_spell.lookup_compound(dataset['text'][i], max_edit_distance)\n",
        "    \n",
        "            for sugestao in sugestoes:\n",
        "                dataset.loc[i, 'text'] = sugestao.term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haLeGaeBrizX"
      },
      "source": [
        "class MetricasClassificacao:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def treinoTeste(self, y_test, pred, algoritmo=None):\n",
        "        if algoritmo == \"lstm\":\n",
        "            import numpy as np\n",
        "            y_test = np.argmax(y_test, axis=1)\n",
        "            pred = np.argmax(pred, axis=1)\n",
        "\n",
        "        MetricasClassificacao().imprimindo(y_test, pred)\n",
        "\n",
        "\n",
        "    def validacaoCruzada(self, model, x, y, folds, y_original=None, algoritmo=None):\n",
        "        from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "        pred = cross_val_predict(model, x, y, cv=folds)\n",
        "\n",
        "        if algoritmo == \"lstm\":\n",
        "            y = y_original\n",
        "\n",
        "        MetricasClassificacao().imprimindo(y, pred)\n",
        "\n",
        "\n",
        "    def imprimindo(self, y, pred):\n",
        "        import numpy as np\n",
        "        from sklearn import metrics\n",
        "        import matplotlib.pyplot as plt\n",
        "        print(\"\\n_____________________________________________________________\")\n",
        "        print(\"\\t\\tRelatorio de Classificação\")\n",
        "        print(\"-------------------------------------------------------------\\n\")\n",
        "        print(metrics.classification_report(y, pred))\n",
        "        print(\"_____________________________________________________________\")\n",
        "        print(\"\\t\\t  Matriz de Confusao\")\n",
        "        print(\"-------------------------------------------------------------\\n\")\n",
        "        print(pd.crosstab(y, pred, rownames=['Real'], colnames=['Predito'], margins=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qQpQMk31iDN"
      },
      "source": [
        "#Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y87tIpZvWiPb"
      },
      "source": [
        "# substituir pelo arquivo do dataset\n",
        "df = FileToDataframe().ConvertToDataframe(\"arquivo do dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxRyCGGviW8C"
      },
      "source": [
        "df = df[df['sent'] != 0] # para remover o sentimento neutro\n",
        "df.reset_index(drop=True, inplace=True)  # restaura os indices de cada linha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxOOq43xf1Ll"
      },
      "source": [
        "!pip install emoji\n",
        "import re, unicodedata, string, emoji\n",
        "\n",
        "for i in range(len(df)):\n",
        "    df.loc[i, 'text'] = re.sub(r\" ?#([^\\s]+)\", \" HASHTAG \", df['text'][i])         # remove hashtag\n",
        "    df.loc[i, 'text'] = emoji.get_emoji_regexp().sub(\" EMOJIS \", df['text'][i])   # remove emojis\n",
        "    df.loc[i, 'text'] = re.sub(\"\\d+|(\\d*\\.\\d+)\",\" NUMBER \", df['text'][i])   # remove numeros\n",
        "    df.loc[i, 'text'] = unicodedata.normalize('NFKD', df['text'][i]).encode('ASCII', 'ignore').decode(\"utf-8\") # remove acentos das palavras\n",
        "    df.loc[i, 'text'] = re.sub(\"\\S*@\\S*\\s?\",\" EMAIL \", df['text'][i])  # remove email\n",
        "    df.loc[i, 'text'] = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', df['text'][i]) # remove pontuacao\n",
        "    df.loc[i, 'text'] = re.sub(\"\\S+\\.com\\S*\",\" SITE \", df['text'][i])  # remove sites\n",
        "    df.loc[i, 'text'] = re.sub(\"\\s+\",\" \", df['text'][i])  # remove espacos desnecessarios"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33DNCPKNT5Gz"
      },
      "source": [
        "CorrecaoOrtografica().corrigir(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-RrORozUB7Y"
      },
      "source": [
        "RemocaoStopwords().stopwordsNLTK(df)\n",
        "#RemocaoStopwords().stopwordsSpacy(df)\n",
        "\n",
        "Stemming().RSLP(df)\n",
        "#Stemming().Snowball(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AltiXV4vD_o"
      },
      "source": [
        "#utilizar o codigo abaixo, pois na remocao das stopwords deixa alguns textos em branco\n",
        "for i in range(len(df)):\n",
        "    if df['text'][i] == \"\":\n",
        "        df = df.drop(i,axis=0)\n",
        "df.reset_index(drop=True, inplace=True)  # restaura os indices de cada linha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYT3PeHT1aOr"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rggnwuSFYLUa"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=0, max_df=0.9, sublinear_tf=True, use_idf=True, ngram_range=(1, 3))\n",
        "classificador = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "model = svm.LinearSVC()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxG1GdnMovBQ"
      },
      "source": [
        "MetricasClassificacao().validacaoCruzada(model, classificador, df['sent'], 10, df['sent'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozjhjPdl1OWX"
      },
      "source": [
        "#LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NAFRNP0wyAN"
      },
      "source": [
        "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import Bidirectional, Conv1D, MaxPooling1D, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "class LSTM_m:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def lstm_artigo(self, x, vocab_size, d):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size,64, input_length=x.shape[1]))\n",
        "        model.add(SpatialDropout1D((0.5)))\n",
        "        model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.5))\n",
        "        model.add(Dense(d))\n",
        "        model.add(Activation(\"sigmoid\"))\n",
        "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def lstm_artigo_vc(self, x, vocab_size, d):   # validacao cruzada\n",
        "        def bm():\n",
        "            model = LSTM_m().lstm_artigo(x, vocab_size, d)\n",
        "            return model\n",
        "        return bm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnsnXlcoTL3W"
      },
      "source": [
        "#utilizar para polaridade ternaria\n",
        "encoding = {0: 0, 1: 1, -1: 2}\n",
        "df['sent'] = [encoding[x] for x in df['sent']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILCcxDyA5k0a"
      },
      "source": [
        "#utilizar para polaridade binaria\n",
        "encoding = {1: 1, -1: 0}\n",
        "df['sent'] = [encoding[x] for x in df['sent']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX471uq2wQ_O"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = Tokenizer(split=' ')\n",
        "tokenizer.fit_on_texts(df['text'].values)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X = tokenizer.texts_to_sequences(df['text'].values)\n",
        "Y = pd.get_dummies(df['sent']).values\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb0LXes5o4Pi"
      },
      "source": [
        "# LSTM com validacao cruzada\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "kmodel = KerasClassifier(build_fn=LSTM_m().lstm_artigo_vc(X, vocab_size, 2), epochs = 5, batch_size=32) # utilizar para polaridade binaria\n",
        "#kmodel = KerasClassifier(build_fn=LSTM_m().lstm_artigo_vc(X, vocab_size, 3), epochs = 5, batch_size=32) # utilizar para polaridade ternaria\n",
        "\n",
        "MetricasClassificacao().validacaoCruzada(kmodel, X, Y, 10, df['sent'], \"lstm\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}